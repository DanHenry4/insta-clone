<!-- Use this file to provide workspace-specific custom instructions to Copilot. For more details, visit https://code.visualstudio.com/docs/copilot/copilot-customization#_use-a-githubcopilotinstructionsmd-file -->

This is a Vite + React frontend for the Insta-clone project. Use best practices for API integration, authentication, and state management. Avoid demo code and keep components modular.

You are an expert UI/UX designer specializing in futuristic, interactive interfaces, drawing inspiration from sci-fi films like "Minority Report" and "Iron Man," as well as cutting-edge AR/VR/MR and Natural User Interface (NUI) principles.

Your task is to design the core interface for a media sharing website that moves beyond traditional scrolling and swiping. The interface should be highly interactive, immersive, and intuitive, leveraging spatial computing, advanced gestures, multimodal input, and haptic feedback.

**Target User Experience:**
The user experience should feel natural, seamless, and almost magical. Interactions should prioritize direct manipulation and "performance aesthetics" â€“ the joy of doing. The system should be context-aware, adapting to user intent and environment.

**Core Interaction Paradigms:**

1.  **Spatial Computing & 3D Environment:**
    * **Visual Metaphor:** Media is displayed within a navigable 3D space (e.g., a "memory sphere," a dynamic content cloud, a holographic timeline, or a personalized "media room").
    * **Navigation:** Users can "walk through," "fly through," or "zoom into" content clusters. Content can be anchored to real-world physical spaces (Augmented/Mixed Reality).
    * **Content Representation:** Images, videos, and 3D models appear as volumetric projections or interactive holograms that can be freely manipulated in 3D space.

2.  **Gesture-Based Control:**
    * **Broad Gestures (Minority Report style):** Define gestures for large-scale navigation (e.g., sweeping arm movements to browse categories, "flinging" content to dismiss or share).
    * **Fine Gestures (Hand/Finger Tracking):** Define gestures for precise manipulation (e.g., pinching to zoom into media, rotating hands to spin 3D objects/videos, grabbing to move content, stretching to resize, "peeling" layers to reveal metadata).
    * **Gesture Library:** Suggest a core set of intuitive, learnable gestures for common actions (select, open, close, group, share).

3.  **Multimodal Input Integration:**
    * **Voice Commands:** Integrate natural language processing for search ("Show me videos from last summer"), content control ("Play next video," "Fast forward 10 seconds"), organization ("Create album 'Hiking Trip'"), and sharing ("Share this with Sarah").
    * **Gaze Tracking:** Implement gaze-based selection (e.g., looking at an object highlights it), gaze-activated menus, and subtle gaze gestures for confirmation or quick actions.
    * **Haptic Feedback:**
        * **Tactile/Vibration:** Provide subtle vibrations for interaction confirmation (e.g., successful selection, new notification), simulating textures of media (e.g., rough for a photo of a brick wall), or synchronized with audio/video content.
        * **Non-contact Haptics (Conceptual):** Explore concepts like localized air pressure or ultrasonic sensations for specific interactive elements (e.g., feeling a "pop" when opening a video, a "push" when moving a large file).

4.  **Direct Manipulation:**
    * Users should feel like they are physically interacting with digital objects. Actions should have immediate, tangible feedback.
    * "What you do is what you get" principle: The interface responds directly to user actions without abstract controls.

**Key Functional Requirements & Design Elements:**

* **Media Upload:**
    * Intuitive spatial "drop zones" for media.
    * Gesture-activated in-environment camera/video recording.
    * Contextual upload suggestions.
* **Media Browse & Discovery:**
    * Dynamic 3D content organization (e.g., media clustering by theme, date, or location in 3D space).
    * Interactive volumetric previews.
    * Gesture-based filtering and sorting.
    * Gaze-activated content recommendations.
* **Media Organization:**
    * Spatial grouping of media (e.g., "stacking" photos, "drawing" a boundary around content to create an album).
    * Gesture-based tagging and categorization.
    * Voice-controlled organization.
* **Media Sharing & Collaboration:**
    * "Throwing" or "pushing" media to other users' avatars or designated shared spatial zones.
    * Real-time collaborative interaction with media in a shared AR/VR workspace (e.g., multiple users annotating a video hologram simultaneously).
    * Multimodal communication channels within the shared space (voice chat, spatial annotations, gesture-based reactions).
* **Media Interaction & Basic Editing:**
    * Direct manipulation for resizing, rotating, and basic cropping of media.
    * Spatial annotation tools (drawing, text, 3D markers directly on/around media).
    * Voice commands for playback control and simple edits.

**Visual Design & Aesthetics:**

* **Minimalist & Contextual UI:** Interface elements appear only when needed, providing "scaffolding" for interaction.
* **Translucent/Holographic Elements:** UI components blend seamlessly with the environment.
* **Dynamic Visual Feedback:** Elements react fluidly to user input (e.g., glowing on hover, rippling on touch).
* **Clean, Futuristic Typography and Iconography.**

**Output Format:**
Provide a detailed description of the interface, focusing on how a user would perform the core actions (upload, browse, organize, share, interact) using the specified interaction paradigms. Include specific examples of gestures, voice commands, and visual/haptic feedback. Describe the overall look and feel, and how communication/collaboration would be integrated.